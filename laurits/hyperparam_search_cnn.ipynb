{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e78399b",
   "metadata": {},
   "source": [
    "# Hyperparameter Search for CNN Model\n",
    "\n",
    "This notebook implements a staged hyperparameter search for the `model_cnn_wss` architecture:\n",
    "\n",
    "1. **Data Preparation**: Load data, split into train/validation/test, then apply subsampling within each split.\n",
    "2. **Stage 1**: Tune learning rate and batch size.\n",
    "3. **Stage 2**: Tune model capacity (number of convolutional blocks and base filter size).\n",
    "4. **Stage 3**: Tune regularization (dropout, weight decay).\n",
    "5. **Stage 4**: Final tuning (learning-rate scheduler, minor kernel tweaks, optional augmentation).\n",
    "\n",
    "Each stage builds on the best settings from previous stages.  \n",
    "Make sure `model_cnn_wss.py` is in the same directory. \n"
   ]
  },
  {
   "cell_type": "code",
   "id": "349751a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T08:15:44.550560Z",
     "start_time": "2025-06-02T08:15:44.542458Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from model_cnn_old import Model_CNN\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "51815a2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T08:17:23.592724Z",
     "start_time": "2025-06-02T08:17:02.493937Z"
    }
   },
   "source": [
    "# Instantiate the Model_CNN to access data loading methods\n",
    "data_path = '/Users/lauritsfauli/PycharmProjects/Final_project_APML/era5'\n",
    "mc = Model_CNN(data_path=data_path)  # adjust args as needed\n",
    "\n",
    "# Load raw data\n",
    "mc.load_data()\n",
    "\n",
    "# Prepare splits without subsampling\n",
    "X = mc.X  # shape (N, C, H, W)\n",
    "y = mc.target  # shape (N,)\n",
    "\n",
    "# 1) First split into train and temp\n",
    "X_train_full, X_temp_full, y_train_full, y_temp_full = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=SEED, shuffle=True\n",
    ")\n",
    "\n",
    "# 2) Split temp into val and test\n",
    "X_val_full, X_test_full, y_val_full, y_test_full = train_test_split(\n",
    "    X_temp_full, y_temp_full, test_size=0.5, random_state=SEED, shuffle=True\n",
    ")\n",
    "\n",
    "print(f'Full shapes:')\n",
    "print(f'  X_train_full: {X_train_full.shape}, y_train_full: {y_train_full.shape}')\n",
    "print(f'  X_val_full:   {X_val_full.shape},   y_val_full:   {y_val_full.shape}')\n",
    "print(f'  X_test_full:  {X_test_full.shape},   y_test_full:   {y_test_full.shape}')\n",
    "\n",
    "# Scale targets on train only\n",
    "scaler = MinMaxScaler()\n",
    "y_train_full = scaler.fit_transform(y_train_full.reshape(-1, 1)).flatten()\n",
    "y_val_full   = scaler.transform(y_val_full.reshape(-1, 1)).flatten()\n",
    "y_test_full  = scaler.transform(y_test_full.reshape(-1, 1)).flatten()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Msl input shape: (1827, 120, 408)\n",
      "input_file_names: ['era5_1940_mean_sea_level_pressure.nc', 'era5_1941_mean_sea_level_pressure.nc', 'era5_1942_mean_sea_level_pressure.nc', 'era5_1943_mean_sea_level_pressure.nc', 'era5_1944_mean_sea_level_pressure.nc']\n",
      "T850 input shape: (1827, 120, 408)\n",
      "input_file_names_t850: ['era5_1940_temperature_850hPa.nc', 'era5_1941_temperature_850hPa.nc', 'era5_1942_temperature_850hPa.nc', 'era5_1943_temperature_850hPa.nc', 'era5_1944_temperature_850hPa.nc']\n",
      "Precipitation target shape: (1827,)\n",
      "target_file_names: ['era5_1940_total_precipitation.nc', 'era5_1941_total_precipitation.nc', 'era5_1942_total_precipitation.nc', 'era5_1943_total_precipitation.nc', 'era5_1944_total_precipitation.nc']\n",
      "No sub-sampling applied.\n",
      "X shape: (1827, 2, 120, 408)\n",
      "Precipitation target shape: (1827,)\n",
      "Full shapes:\n",
      "  X_train_full: (1278, 2, 120, 408), y_train_full: (1278,)\n",
      "  X_val_full:   (274, 2, 120, 408),   y_val_full:   (274,)\n",
      "  X_test_full:  (275, 2, 120, 408),   y_test_full:   (275,)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dac5f9a80c9f7a9a"
  },
  {
   "cell_type": "code",
   "id": "82c645e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T08:17:32.624256Z",
     "start_time": "2025-06-02T08:17:32.488562Z"
    }
   },
   "source": [
    "# Define subsampling function (spatial patches)\n",
    "def sample_data(X_array, n):\n",
    "    # Example: split each sample into 'n' spatial patches along the first spatial dimension.\n",
    "    # Modify as needed for your specific subsampling logic.\n",
    "    # Here, assume X_array shape: (M, C, H, W). We subsample H into n chunks of size H//n.\n",
    "    M, C, H, W = X_array.shape\n",
    "    assert H % n == 0, \"H must be divisible by subsample_dim\"\n",
    "    h_chunk = H // n\n",
    "    collected = []\n",
    "    for i in range(n):\n",
    "        patch = X_array[:, :, i*h_chunk:(i+1)*h_chunk, :]\n",
    "        collected.append(patch)\n",
    "    # Stack patches and flatten along sample axis\n",
    "    concatenated = np.concatenate(collected, axis=0)\n",
    "    return concatenated\n",
    "\n",
    "# Example: verify shapes\n",
    "X_train_sub_example = sample_data(X_train_full, n=4)\n",
    "print(f'After subsampling train_full with n=4: {X_train_sub_example.shape}')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After subsampling train_full with n=4: (5112, 2, 30, 408)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "76107272",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T08:17:36.236005Z",
     "start_time": "2025-06-02T08:17:36.232120Z"
    }
   },
   "source": [
    "def prepare_datasets(X_train, y_train, X_val, y_val, X_test, y_test, sub_sampling=False, sub_sample_dim=4):\n",
    "    # Optionally apply subsampling within each split\n",
    "    if sub_sampling:\n",
    "        X_train_ds = sample_data(X_train, sub_sample_dim)\n",
    "        X_val_ds   = sample_data(X_val,   sub_sample_dim)\n",
    "        X_test_ds  = sample_data(X_test,  sub_sample_dim)\n",
    "        # Repeat targets\n",
    "        y_train_ds = np.repeat(y_train, sub_sample_dim)\n",
    "        y_val_ds   = np.repeat(y_val,   sub_sample_dim)\n",
    "        y_test_ds  = np.repeat(y_test,  sub_sample_dim)\n",
    "    else:\n",
    "        X_train_ds, y_train_ds = X_train, y_train\n",
    "        X_val_ds,   y_val_ds   = X_val,   y_val\n",
    "        X_test_ds,  y_test_ds  = X_test,  y_test\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_train_t = torch.from_numpy(X_train_ds).float()\n",
    "    y_train_t = torch.from_numpy(y_train_ds).float().unsqueeze(1)\n",
    "    X_val_t   = torch.from_numpy(X_val_ds).float()\n",
    "    y_val_t   = torch.from_numpy(y_val_ds).float().unsqueeze(1)\n",
    "    X_test_t  = torch.from_numpy(X_test_ds).float()\n",
    "    y_test_t  = torch.from_numpy(y_test_ds).float().unsqueeze(1)\n",
    "\n",
    "    return (X_train_t, y_train_t), (X_val_t, y_val_t), (X_test_t, y_test_t)\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "d12b5fe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T08:17:52.536765Z",
     "start_time": "2025-06-02T08:17:52.530512Z"
    }
   },
   "source": [
    "def train_and_evaluate(hparams, train_data, val_data, test_data):\n",
    "    # Unpack hyperparameters\n",
    "    lr = hparams['lr']\n",
    "    batch_size = hparams['batch_size']\n",
    "    conv_cfg = hparams['conv_cfg']   # list of tuples\n",
    "    fc_cfg = hparams['fc_cfg']       # list of ints\n",
    "    drop = hparams['dropout']\n",
    "    weight_decay = hparams['weight_decay']\n",
    "\n",
    "    # Instantiate a fresh Model_CNN and build the model\n",
    "    mc = Model_CNN(data_path='./data', latitude_range=(...), longitude_range=(...))\n",
    "    mc.X = X_train_full  # stub to allow model building\n",
    "    mc.build_model(dropout_rate=drop, conv_layers=conv_cfg, fc_layers=fc_cfg)\n",
    "    model = mc.model.to(device)\n",
    "\n",
    "    # DataLoaders\n",
    "    (X_tr, y_tr), (X_v, y_v), (X_te, y_te) = train_data, val_data, test_data\n",
    "    train_loader = DataLoader(TensorDataset(X_tr, y_tr), batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(TensorDataset(X_v, y_v), batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(TensorDataset(X_te, y_te), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Loss & optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Training loop (simple fixed epochs)\n",
    "    num_epochs = 20\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                preds = model(xb)\n",
    "                val_losses.append(criterion(preds, yb).item())\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    # Load best and evaluate on test\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            test_losses.append(criterion(preds, yb).item())\n",
    "    avg_test_loss = np.mean(test_losses)\n",
    "    return best_val_loss, avg_test_loss\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "554a1e96",
   "metadata": {},
   "source": [
    "## Stage 1: Learning Rate & Batch Size\n",
    "\n",
    "We keep a fixed architecture (3 conv blocks with filters [32, 64, 128], kernels=3, pooling=2; FC sizes [256, 128]) and vary:\n",
    "- Learning rate: [1e-4, 3e-4, 1e-3, 3e-3]\n",
    "- Batch size: [16, 32, 64]\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "efb5507e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T08:26:38.710166Z",
     "start_time": "2025-06-02T08:17:57.013032Z"
    }
   },
   "source": [
    "# Define fixed architecture for Stage 1\n",
    "base_conv_cfg = [(32, 3, 2), (64, 3, 2), (128, 3, 2)]\n",
    "base_fc_cfg   = [256, 128]\n",
    "\n",
    "lrs = [1e-4, 3e-4, 1e-3, 3e-3]\n",
    "batch_sizes = [16, 32, 64]\n",
    "\n",
    "results_stage1 = []\n",
    "\n",
    "for lr in lrs:\n",
    "    for bs in batch_sizes:\n",
    "        hparams = {\n",
    "            'lr': lr,\n",
    "            'batch_size': bs,\n",
    "            'conv_cfg': base_conv_cfg,\n",
    "            'fc_cfg': base_fc_cfg,\n",
    "            'dropout': 0.2,\n",
    "            'weight_decay': 1e-5\n",
    "        }\n",
    "        # Prepare data (with subsampling if desired; here False)\n",
    "        train_data, val_data, test_data = prepare_datasets(\n",
    "            X_train_full, y_train_full,\n",
    "            X_val_full,   y_val_full,\n",
    "            X_test_full,  y_test_full,\n",
    "            sub_sampling=False, sub_sample_dim=4\n",
    "        )\n",
    "        val_loss, test_loss = train_and_evaluate(hparams, train_data, val_data, test_data)\n",
    "        results_stage1.append((lr, bs, val_loss, test_loss))\n",
    "        print(f\"LR {lr}, BS {bs} -> Val: {val_loss:.4f}, Test: {test_loss:.4f}\")\n",
    "\n",
    "# Save to DataFrame\n",
    "df_stage1 = pd.DataFrame(results_stage1, columns=['lr', 'batch_size', 'val_loss', 'test_loss'])\n",
    "df_stage1"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary: ==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "FlexibleCNN                              [1, 1]                    --\n",
      "├─Sequential: 1-1                        [1, 128, 15, 51]          --\n",
      "│    └─Conv2d: 2-1                       [1, 32, 120, 408]         608\n",
      "│    └─BatchNorm2d: 2-2                  [1, 32, 120, 408]         64\n",
      "│    └─ReLU: 2-3                         [1, 32, 120, 408]         --\n",
      "│    └─MaxPool2d: 2-4                    [1, 32, 60, 204]          --\n",
      "│    └─Dropout2d: 2-5                    [1, 32, 60, 204]          --\n",
      "│    └─Conv2d: 2-6                       [1, 64, 60, 204]          18,496\n",
      "│    └─BatchNorm2d: 2-7                  [1, 64, 60, 204]          128\n",
      "│    └─ReLU: 2-8                         [1, 64, 60, 204]          --\n",
      "│    └─MaxPool2d: 2-9                    [1, 64, 30, 102]          --\n",
      "│    └─Dropout2d: 2-10                   [1, 64, 30, 102]          --\n",
      "│    └─Conv2d: 2-11                      [1, 128, 30, 102]         73,856\n",
      "│    └─BatchNorm2d: 2-12                 [1, 128, 30, 102]         256\n",
      "│    └─ReLU: 2-13                        [1, 128, 30, 102]         --\n",
      "│    └─MaxPool2d: 2-14                   [1, 128, 15, 51]          --\n",
      "│    └─Dropout2d: 2-15                   [1, 128, 15, 51]          --\n",
      "├─Sequential: 1-2                        [1, 1]                    --\n",
      "│    └─Linear: 2-16                      [1, 256]                  25,067,776\n",
      "│    └─BatchNorm1d: 2-17                 [1, 256]                  512\n",
      "│    └─ReLU: 2-18                        [1, 256]                  --\n",
      "│    └─Dropout: 2-19                     [1, 256]                  --\n",
      "│    └─Linear: 2-20                      [1, 128]                  32,896\n",
      "│    └─BatchNorm1d: 2-21                 [1, 128]                  256\n",
      "│    └─ReLU: 2-22                        [1, 128]                  --\n",
      "│    └─Dropout: 2-23                     [1, 128]                  --\n",
      "│    └─Linear: 2-24                      [1, 1]                    129\n",
      "==========================================================================================\n",
      "Total params: 25,194,977\n",
      "Trainable params: 25,194,977\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 507.26\n",
      "==========================================================================================\n",
      "Input size (MB): 0.39\n",
      "Forward/backward pass size (MB): 43.87\n",
      "Params size (MB): 100.78\n",
      "Estimated Total Size (MB): 145.05\n",
      "==========================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 27\u001B[39m\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m# Prepare data (with subsampling if desired; here False)\u001B[39;00m\n\u001B[32m     21\u001B[39m train_data, val_data, test_data = prepare_datasets(\n\u001B[32m     22\u001B[39m     X_train_full, y_train_full,\n\u001B[32m     23\u001B[39m     X_val_full,   y_val_full,\n\u001B[32m     24\u001B[39m     X_test_full,  y_test_full,\n\u001B[32m     25\u001B[39m     sub_sampling=\u001B[38;5;28;01mFalse\u001B[39;00m, sub_sample_dim=\u001B[32m4\u001B[39m\n\u001B[32m     26\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m val_loss, test_loss = \u001B[43mtrain_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     28\u001B[39m results_stage1.append((lr, bs, val_loss, test_loss))\n\u001B[32m     29\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mLR \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlr\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, BS \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m -> Val: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, Test: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 34\u001B[39m, in \u001B[36mtrain_and_evaluate\u001B[39m\u001B[34m(hparams, train_data, val_data, test_data)\u001B[39m\n\u001B[32m     32\u001B[39m xb, yb = xb.to(device), yb.to(device)\n\u001B[32m     33\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m34\u001B[39m preds = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     35\u001B[39m loss = criterion(preds, yb)\n\u001B[32m     36\u001B[39m loss.backward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AppliedML2025/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AppliedML2025/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Final_project_APML/LRP_ERA5_fork/laurits/model_cnn_old.py:248\u001B[39m, in \u001B[36mModel_CNN.build_model.<locals>.FlexibleCNN.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    247\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m--> \u001B[39m\u001B[32m248\u001B[39m     x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconv_layers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    249\u001B[39m     x = x.view(-\u001B[32m1\u001B[39m, \u001B[38;5;28mself\u001B[39m.flat_features)\n\u001B[32m    250\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.fc_layers(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AppliedML2025/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AppliedML2025/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AppliedML2025/venv/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    248\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[32m    249\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m         \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    251\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AppliedML2025/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AppliedML2025/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AppliedML2025/venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:213\u001B[39m, in \u001B[36mMaxPool2d.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    212\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor):\n\u001B[32m--> \u001B[39m\u001B[32m213\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmax_pool2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    214\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    215\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mkernel_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    216\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    217\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    218\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[43m        \u001B[49m\u001B[43mceil_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mceil_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    220\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_indices\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mreturn_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    221\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AppliedML2025/venv/lib/python3.12/site-packages/torch/_jit_internal.py:624\u001B[39m, in \u001B[36mboolean_dispatch.<locals>.fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    622\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m if_true(*args, **kwargs)\n\u001B[32m    623\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m624\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mif_false\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AppliedML2025/venv/lib/python3.12/site-packages/torch/nn/functional.py:830\u001B[39m, in \u001B[36m_max_pool2d\u001B[39m\u001B[34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001B[39m\n\u001B[32m    828\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m stride \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    829\u001B[39m     stride = torch.jit.annotate(List[\u001B[38;5;28mint\u001B[39m], [])\n\u001B[32m--> \u001B[39m\u001B[32m830\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmax_pool2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mceil_mode\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "17758d7b",
   "metadata": {},
   "source": [
    "## Stage 2: Model Capacity (Depth & Width)\n",
    "\n",
    "Using best LR & BS from Stage 1, vary:\n",
    "- Number of conv blocks: [2, 3, 4]\n",
    "- Base filter size: [16, 32]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c46fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick best LR & BS from df_stage1\n",
    "best = df_stage1.sort_values('val_loss').iloc[0]\n",
    "best_lr = best['lr']\n",
    "best_bs = best['batch_size']\n",
    "print(f\"Best from Stage1: lr={best_lr}, bs={best_bs}\")\n",
    "\n",
    "# Stage 2 search\n",
    "depths = [2, 3, 4]\n",
    "base_filters = [16, 32]\n",
    "results_stage2 = []\n",
    "\n",
    "for depth in depths:\n",
    "    for bf in base_filters:\n",
    "        # Build conv config\n",
    "        conv_cfg = []\n",
    "        for i in range(depth):\n",
    "            out_ch = bf * (2**i)\n",
    "            conv_cfg.append((out_ch, 3, 2))\n",
    "        hparams = {\n",
    "            'lr': best_lr,\n",
    "            'batch_size': best_bs,\n",
    "            'conv_cfg': conv_cfg,\n",
    "            'fc_cfg': [256, 128],\n",
    "            'dropout': 0.2,\n",
    "            'weight_decay': 1e-5\n",
    "        }\n",
    "        train_data, val_data, test_data = prepare_datasets(\n",
    "            X_train_full, y_train_full,\n",
    "            X_val_full,   y_val_full,\n",
    "            X_test_full,  y_test_full,\n",
    "            sub_sampling=False, sub_sample_dim=4\n",
    "        )\n",
    "        val_loss, test_loss = train_and_evaluate(hparams, train_data, val_data, test_data)\n",
    "        results_stage2.append((depth, bf, val_loss, test_loss))\n",
    "        print(f\"Depth {depth}, BaseFilt {bf} -> Val: {val_loss:.4f}, Test: {test_loss:.4f}\")\n",
    "\n",
    "df_stage2 = pd.DataFrame(results_stage2, columns=['depth', 'base_filters', 'val_loss', 'test_loss'])\n",
    "df_stage2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364c47b",
   "metadata": {},
   "source": [
    "## Stage 3: Regularization (Dropout & Weight Decay)\n",
    "\n",
    "Using best architecture from Stage 2, vary:\n",
    "- Dropout: [0.1, 0.2, 0.3]\n",
    "- Weight decay: [1e-4, 1e-5, 1e-6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68011ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick best architecture from Stage2\n",
    "best2 = df_stage2.sort_values('val_loss').iloc[0]\n",
    "best_depth = int(best2['depth'])\n",
    "best_bf = int(best2['base_filters'])\n",
    "print(f\"Best from Stage2: depth={best_depth}, base_filters={best_bf}\")\n",
    "\n",
    "# Reconstruct conv_cfg for Stage 3\n",
    "conv_cfg_best = [(best_bf * (2**i), 3, 2) for i in range(best_depth)]\n",
    "fc_cfg_best = [256, 128]\n",
    "\n",
    "dropouts = [0.1, 0.2, 0.3]\n",
    "wds = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "results_stage3 = []\n",
    "\n",
    "for drop in dropouts:\n",
    "    for wd in wds:\n",
    "        hparams = {\n",
    "            'lr': best_lr,\n",
    "            'batch_size': best_bs,\n",
    "            'conv_cfg': conv_cfg_best,\n",
    "            'fc_cfg': fc_cfg_best,\n",
    "            'dropout': drop,\n",
    "            'weight_decay': wd\n",
    "        }\n",
    "        train_data, val_data, test_data = prepare_datasets(\n",
    "            X_train_full, y_train_full,\n",
    "            X_val_full,   y_val_full,\n",
    "            X_test_full,  y_test_full,\n",
    "            sub_sampling=False, sub_sample_dim=4\n",
    "        )\n",
    "        val_loss, test_loss = train_and_evaluate(hparams, train_data, val_data, test_data)\n",
    "        results_stage3.append((drop, wd, val_loss, test_loss))\n",
    "        print(f\"Drop {drop}, WD {wd} -> Val: {val_loss:.4f}, Test: {test_loss:.4f}\")\n",
    "\n",
    "df_stage3 = pd.DataFrame(results_stage3, columns=['dropout', 'weight_decay', 'val_loss', 'test_loss'])\n",
    "df_stage3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f953758",
   "metadata": {},
   "source": [
    "## Stage 4: Final Tuning – Learning Rate Scheduler\n",
    "\n",
    "Take the best config from Stage 3 and try a OneCycle scheduler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5dda12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick best reg settings from Stage3\n",
    "best3 = df_stage3.sort_values('val_loss').iloc[0]\n",
    "best_drop = best3['dropout']\n",
    "best_wd = best3['weight_decay']\n",
    "print(f\"Best from Stage3: dropout={best_drop}, weight_decay={best_wd}\")\n",
    "\n",
    "hparams = {\n",
    "    'lr': best_lr,  # We'll modify with scheduler\n",
    "    'batch_size': best_bs,\n",
    "    'conv_cfg': conv_cfg_best,\n",
    "    'fc_cfg': fc_cfg_best,\n",
    "    'dropout': best_drop,\n",
    "    'weight_decay': best_wd\n",
    "}\n",
    "\n",
    "# Define training with OneCycleLR\n",
    "def train_with_onecycle(hparams, train_data, val_data, test_data, max_lr, epochs=20):\n",
    "    # Build model\n",
    "    mc = Model_CNN(data_path='./data', latitude_range=(...), longitude_range=(...))\n",
    "    mc.X = X_train_full\n",
    "    mc.build_model(dropout_rate=hparams['dropout'],\n",
    "                   conv_layers=hparams['conv_cfg'],\n",
    "                   fc_layers=hparams['fc_cfg'])\n",
    "    model = mc.model.to(device)\n",
    "\n",
    "    (X_tr, y_tr), (X_v, y_v), (X_te, y_te) = train_data, val_data, test_data\n",
    "    train_loader = DataLoader(TensorDataset(X_tr, y_tr), batch_size=hparams['batch_size'], shuffle=True)\n",
    "    val_loader   = DataLoader(TensorDataset(X_v, y_v), batch_size=hparams['batch_size'], shuffle=False)\n",
    "    test_loader  = DataLoader(TensorDataset(X_te, y_te), batch_size=hparams['batch_size'], shuffle=False)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=hparams['lr'], weight_decay=hparams['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=max_lr, epochs=epochs, steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                preds = model(xb)\n",
    "                val_losses.append(criterion(preds, yb).item())\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_onecycle.pth')\n",
    "\n",
    "    model.load_state_dict(torch.load('best_onecycle.pth'))\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            test_losses.append(criterion(preds, yb).item())\n",
    "    return best_val_loss, np.mean(test_losses)\n",
    "\n",
    "results_stage4 = []\n",
    "for max_lr in [best_lr * 10, best_lr * 5, best_lr * 2]:\n",
    "    train_data, val_data, test_data = prepare_datasets(\n",
    "        X_train_full, y_train_full,\n",
    "        X_val_full,   y_val_full,\n",
    "        X_test_full,  y_test_full,\n",
    "        sub_sampling=False, sub_sample_dim=4\n",
    "    )\n",
    "    val_loss, test_loss = train_with_onecycle(hparams, train_data, val_data, test_data, max_lr)\n",
    "    results_stage4.append((max_lr, val_loss, test_loss))\n",
    "    print(f\"max_lr {max_lr} -> Val: {val_loss:.4f}, Test: {test_loss:.4f}\")\n",
    "\n",
    "df_stage4 = pd.DataFrame(results_stage4, columns=['max_lr', 'val_loss', 'test_loss'])\n",
    "df_stage4"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
